# -*- coding: utf-8 -*-
"""Actividad1MachineLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u_vqv9hZXCR7oWEPNUQVV-c5-zSYObLU

#**<center>Universidad Autónoma de Chihuahua**

<div align= "center">
<img src="https://uach.mx/assets/media/snippet/62/escudo-web-header-black.svg" width="350" height="200" />

</div>
<div align ="center">
<img src="https://cimav.edu.mx/wp-content/uploads/2017/05/ingenieria-uach.png" width="200" height="200" />
</div>

##<center>Maestría en ingeniería en computación
##<center>Machine Learning
##<center>Trabajo: End to End Project

###<center>Estudiante:Ing. Adela Guadalupe Almazán López</center>




<br/>
<br/>
<br/>
<br/>


<h5>
<center>
19 de Septiembre de 2023
<br/>
Chihuahua, Chihuahua.
</center>
</center>
</h5>


<br/>

Importar librerias
"""

import os
import tarfile
import urllib
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit

"""Montar unidad del Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Dirección del archivo"""

archivo_path = '/content/drive/MyDrive/MIC_2SEMESTRE_MACHINE_LEARNING/housing.csv'

"""Cargar datos desde un archivo CSV en DataFrame de pandas

### 2 Get Data
"""

housing = pd.read_csv(archivo_path, index_col=False)
housing.head()

housing.columns  ###Columnas

housing.info()  ###207 total bedrooms

####Información

###Contar valores de esa columna

housing["ocean_proximity"].value_counts()

###Dar valires de referencia
###Cantidad de registros
housing.describe()

housing.hist(bins=50, figsize=(20,15))
plt.show###Graficar

"""Dividir los datos

### 3 Exploración de datos
"""

def split_train_test(data, test_ratio):
    shuffled_indices =np.random.permutation(len(data)) ###Aleatorios tamaño del data
    test_set_size = int (len(data) * test_ratio)
    ###set test
    test_indices=shuffled_indices[:test_set_size]

    ###set de entrenamiento
    train_indices=shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = split_train_test(housing, 0.2)

len(train_set)

len(test_set)



###puede hacer repdocudcirble el conjunto de datos
train_set_sk, test_set_sk =train_test_split(housing, test_size=0.2, random_state=42)

len(train_set_sk)

len(test_set_sk)

housing["income_cat"]= pd.cut(housing["median_income"],bins=[0.,1.5,3.0,4.5,6., np.inf],labels=[1,2,3,4,5])

housing.head()

housing["income_cat"].hist()

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]

strat_test_set["income_cat"].value_counts() / len(strat_test_set)

housing["income_cat"].value_counts()/len(housing)

for set_ in(strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)

#split=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
#for train_index, test_index in split(housing, housing["income_cat"]):
   # strat_test_set =hpusing.loc[text_index]

housing=strat_train_set.copy()

housing.head()

housing.plot(kind="scatter", x="longitude", y="latitude")

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"]/100, label="population", figsize=(10,7), c="median_house_value", cmap="jet", colorbar=True)
plt.legend

numeric_columns = housing.select_dtypes(include=[np.number])
corr_matrix = numeric_columns.corr()

corr_matrix["median_house_value"].sort_values(ascending=False)

from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]

scatter_matrix(housing[attributes], figsize=(12, 8))

housing["rooms_per__housenhold"]=housing["total_rooms"]/housing["households"]
housing["bedrooms_per_room"]=housing["total_bedrooms"]/housing["total_rooms"]
housing["population_per_household"]=housing["population"]/housing["households"]

numeric_columns = housing.select_dtypes(include=[np.number])
corr_matrix = numeric_columns.corr()



##Preparar los datos para algoritmos de ML
housing = strat_train_set.drop("median_house_value", axis=1)
housing_labels = strat_train_set["median_house_value"].copy()

housing.head()

housing_labels.head()

sample_incomplete = housing[housing.isnull().any(axis=1)].head()

sample_incomplete

housing.dropna(subset=["total_bedrooms"])   ###Option ---> get rid of the correesponding distracts
housing.drop("total_bedrooms", axis=1)
median=housing["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inplace=True)

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")

housing_num = housing.drop("ocean_proximity", axis=1)
housing_num.head()

imputer.fit(housing_num)

imputer.statistics_

housing_num.median().values

X=imputer.transform(housing_num)

print(X)

## Put it back into a pandas
housing_tr= pd.DataFrame(X, columns=housing_num.columns,index=housing_num.index)

housing_tr.head()

housing_tr[housing_tr.isnull().any(axis=1)].head()

##handling text and Categorical atributtes
housing_cat= housing[["ocean_proximity"]]
housing_cat.head(10)

housing[["ocean_proximity"]].value_counts()

## convert these categories
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder= OrdinalEncoder()
housing_cat_encoded=ordinal_encoder.fit_transform(housing_cat)
housing_cat_encoded[:10]

ordinal_encoder.categories_

#funcion para  onehotencoder
from sklearn.preprocessing import OneHotEncoder
cat_encoder=OneHotEncoder()
housing_cat_1hot= cat_encoder.fit_transform(housing_cat)
housing_cat_1hot

housing_cat_1hot.toarray()

cat_encoder.categories_

#custom transfomers: here  is a small transformer class that adds the combined attributes:

from sklearn.base import BaseEstimator, TransformerMixin

# column index
rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self  # nothing else to do
    def transform(self, X):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.values)

housing_extra_attribs

housing_extra_attribs=pd.DataFrame(housing_extra_attribs, columns=list(housing.columns)+["rooms_per_household", "population_per_household"],
    index=housing.index)
housing_extra_attribs.head()

"""### 4. Preparar los datos para el modelo"""

#TRANSFORMATION PIPELINES
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline=Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("attrins_adder", CombinedAttributesAdder()),
    ("std_scaler", StandardScaler()),
])

housing_num_tr= num_pipeline.fit_transform(housing_num)

housing_num_tr

tt= pd.DataFrame(housing_num_tr, columns=list(housing.columns)+["rooms_per_household", "population_per_household"],
    index=housing.index)

tt.head()

## ColumnTransformer

from sklearn.compose import ColumnTransformer

num_attribs= list(housing_num)
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer([
    ("num", num_pipeline,num_attribs),
    ("cat", OneHotEncoder(),cat_attribs),
])

housing_prepared = full_pipeline.fit_transform(housing)

housing.head()

housing_prepared.shape

pd.DataFrame(housing_prepared, columns=list(housing.columns)+["rooms_per_household", "population_per_household", " bedrooms_per_room","x1","x2","x3","x4"],
    index=housing.index)

"""### Linear Regression"""

# Training and evaluating on the training set
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

some_data =housing.iloc[:5]
some_labels =housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)
print("Predictions:", lin_reg.predict(some_data_prepared))

print("labels:", list(some_labels))

from sklearn.metrics import mean_squared_error
housing_predictions = lin_reg.predict(housing_prepared)
lin_mse=mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse

"""### Decision Tree"""

# More complex model ejemplo. treeregressor
from sklearn.tree import DecisionTreeRegressor

tree_reg= DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels)

#now the model is trained, must be evaluated
housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels,housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse

##Better Evaluation Using Cross-Validation
from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree_reg,housing_prepared, housing_labels,
                        scoring = "neg_mean_squared_error", cv=10)
tree_rmse_scores=np.sqrt(-scores)

def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())

display_scores(tree_rmse_scores)

lin_scores= cross_val_score(lin_reg,housing_prepared, housing_labels,  scoring="neg_mean_squared_error",cv=10)
lin_rmse_scores=np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)

"""### RandomForestRegressor"""

## RandomForest
from sklearn.ensemble import RandomForestRegressor
forest_reg = RandomForestRegressor()
forest_reg.fit(housing_prepared,housing_labels)

housing_predictions = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, housing_predictions)
forest_rmse = np.sqrt(forest_mse)
forest_rmse

from sklearn.model_selection import cross_val_score

forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,
                                scoring="neg_mean_squared_error", cv=10)
forest_rmse_scores = np.sqrt(-forest_scores)
display_scores(forest_rmse_scores)

"""### 6 Fine tunning del modelo
Ajustar parametros
"""

##FINE-TUNE YOUR MODEL
##GRID SEARCH

from sklearn.model_selection import GridSearchCV

param_grid= [{"n_estimators": [3,10,30], "max_features": [2,4,6,8]},
            {"bootstrap": [False], "n_estimators":[3,10],"max_features":[2,3,4]},]
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                          scoring= "neg_mean_squared_error",
                          return_train_score=True)

grid_search.fit(housing_prepared,housing_labels)

grid_search.best_params_

grid_search.best_estimator_

cvres=grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score),params)

feature_importances = grid_search.best_estimator_.feature_importances_
feature_importances

extra_attribs= ["room_per_hhold","pop_per_hhold","bedrooms_per_room"]
cat_encoder=full_pipeline.named_transformers_["cat"]
cat_one_hot_attribs=list(cat_encoder.categories_[0])
attributes=num_attribs+extra_attribs+cat_one_hot_attribs
sorted(zip(feature_importances,attributes),reverse=True)

final_model=grid_search.best_estimator_
X_test=strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()
X_test_prepared= full_pipeline.transform(X_test)

##Evaluate your system on the Test Set

final_predictions = final_model.predict(X_test_prepared)
final_mse = mean_squared_error(y_test, final_predictions)
final_rmse =np.sqrt(final_mse)

from scipy import stats
confidence=0.95
square_errors =(final_predictions - y_test)**2
np.sqrt(stats.t.interval(confidence, len(square_errors)-1,
                        loc= square_errors.mean(),
                        scale=stats.sem(square_errors)))







